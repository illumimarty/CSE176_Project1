# -*- coding: utf-8 -*-
"""project2-groupB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h5-a9GSEqL13Ur952mnuY_vCH0ARDyF0
"""

# ! git clone https://github.com/UCMerced-ML/LC-model-compression

# ! pip3 install -e ./LC-model-compression

"""Import dependencies"""

import lc
import torch
import numpy as np
from torch import nn, optim
from tqdm import tqdm
from torchvision import datasets
from torchvision.transforms import ToTensor
from torch.utils.data import DataLoader, random_split, Subset, TensorDataset, ConcatDataset
from sklearn.model_selection import KFold

def compute_acc_loss(forward_func, data_loader):
    correct_cnt, ave_loss = 0, 0
    for batch_idx, (x, target) in enumerate(data_loader):
        with torch.no_grad():
            target = target.cuda()
            score, loss = forward_func(x.cuda(), target)
            _, pred_label = torch.max(score.data, 1)
            correct_cnt += (pred_label == target.data).sum().item()
            ave_loss += loss.data.item() * len(x)
    accuracy = correct_cnt * 1.0 / len(data_loader.dataset)
    ave_loss /= len(data_loader.dataset)
    return accuracy, ave_loss

"""Getting digit subset from MNIST (1, 2, 4, 7, 8)"""

def toDataLoader(subset):

    # remapping values corresponding to indexes
    labels = []
    for i in range(len(subset)):
        if subset[i][1] == 1:
            labels.append(0)
        if subset[i][1] == 2:
            labels.append(1)
        if subset[i][1] == 4:
            labels.append(2)
        if subset[i][1] == 7:
            labels.append(3)
        if subset[i][1] == 8:
            labels.append(4)
            
    labels = torch.tensor(labels)

    # Normalizing data
    data_loader = DataLoader(subset, batch_size=len(subset))
    numpy_data = next(iter(data_loader))[0].numpy().reshape([-1,28*28]).astype(np.float32)
    data_norm = (numpy_data / 255).astype(np.float32)
    data_mean = data_norm.mean(axis=0)
    data_norm -= data_mean
    combined_data = TensorDataset(torch.from_numpy(data_norm), labels)
    data_loader = DataLoader(combined_data, batch_size=len(combined_data), shuffle=True)

    return data_loader

def getDataset(digits, data, split=False):
    digit_list = []
    subset_range = []

    # Getting features of the specified digits from MNIST
    for digit in digits:
        digit_list.append(1*(data.targets == digit).nonzero().flatten().tolist())

    for digit in digit_list:
        subset_range += digit

    # Getting labels
    data_subset = Subset(data, subset_range)

    if split is True:
        train_len = int(0.8*len(data_subset))
        val_len = len(data_subset) - train_len

        train_subset, valid_subset = random_split(data_subset, [train_len, val_len], generator=torch.Generator().manual_seed(1))

        train_loader = toDataLoader(train_subset)
        valid_loader = toDataLoader(valid_subset)

        return train_loader, valid_loader

    else:
        data_loader = toDataLoader(data_subset)
        return data_loader

digits = [1,2,4,7,8]
train_data_th = datasets.MNIST(root='./datasets', download=True, train=True, transform=ToTensor())
test_data_th = datasets.MNIST(root='./datasets', download=True, train=False, transform=ToTensor())

train_loader, valid_loader = getDataset(digits, train_data_th, split=True)
# test_loader = getDataset(digits, test_data_th, split=False)

"""Defining network architecture"""

device = torch.device('cuda')

class ReferenceNet(nn.Module):
    def __init__(self):
        super(ReferenceNet,self).__init__()
        # Input layer, transform the image to 500 neurons 
        self.fc1 = nn.Linear(28*28, 500)
        # Hidden layer 1 -> 2, 500 neurons to 300 neurons
        self.fc2 = nn.Linear(500, 300)
        # Hidden layer 2 -> Output layer, 300 neurons to 5 ouput classes
        self.fc3 = nn.Linear(300, 5)

        # Defining the activation function as ReLu (consider using the softmax function, multi-class)
        self.relu = nn.ReLU()
        # Softmax can be used on the output layer
        self.softmax = nn.Softmax()

    def forward(self, x):
        # Flattens the image into an object of the following dimensions: (batch_size x 784)
        x = x.view(-1,28*28)
        #print(x.size())
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.softmax(self.fc3(x))
        return x

# def train_test_acc_eval_f(net):
#     train_loader, test_loader = data_loader()
#     def forward_func(x, target):
#         y = net(x)
#         return y, net.loss(y, target)
#     acc_train, loss_train = compute_acc_loss(forward_func, train_loader)
#     acc_test, loss_test = compute_acc_loss(forward_func, test_loader)

#     print(f"Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}")
#     print(f"TEST ERR: {100-acc_test*100:.2f}%, test loss: {loss_test}")
    
# def load_reference_network():
#     net = lenet300_modern().to(device)
#     # state_dict = torch.load('lenet300_modern_drop_wd.th')
#     state_dict = torch.utils.model_zoo.load_url('https://ucmerced.box.com/shared/static/766axnc8qq429hiqqyqqo07ek46oqoxq.th')
#     net.load_state_dict(state_dict)
#     return net

"""k-folds CV setup"""

def train_epoch(model,device,dataloader,loss_fn,optimizer):
    train_loss,train_correct=0.0,0
    model.train()
    for images, labels in dataloader:

        images,labels = images.to(device),labels.to(device)
        optimizer.zero_grad()
        output = model(images)
        loss = loss_fn(output,labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * images.size(0)
        scores, predictions = torch.max(output.data, 1)
        train_correct += (predictions == labels).sum().item()

    return train_loss

def valid_epoch(model,device,dataloader,loss_fn):
    valid_loss, val_correct = 0.0, 0
    model.eval()
    for images, labels in dataloader:

        images,labels = images.to(device),labels.to(device)
        output = model(images)
        loss=loss_fn(output,labels)
        valid_loss+=loss.item()*images.size(0)
        scores, predictions = torch.max(output.data,1)
        val_correct+=(predictions == labels).sum().item()

    return valid_loss,val_correct

torch.manual_seed(42)
criterion = nn.CrossEntropyLoss()

num_epochs=10
batch_size=1024
k=10
splits=KFold(n_splits=k,shuffle=True,random_state=42)
foldperf={}

dataset = ConcatDataset([train_data_th, test_data_th])

for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):

    print('Fold {}'.format(fold + 1))


    # train_sampler = SubsetRandomSampler(train_idx)
    # test_sampler = SubsetRandomSampler(val_idx)
    # train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)
    # test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)
    
    # device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    
    model = ReferenceNet()
    # model.to(device)
    # optimizer = optim.Adam(model.parameters(), lr=0.002)
    params = list(filter(lambda p: p.requires_grad, model.parameters()))
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    optimizer = optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True)

    history = {'train_loss': [], 'valid_loss': [],'train_acc':[],'valid_acc':[]}

    for epoch in range(num_epochs):
        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)
        valid_loss, valid_correct=valid_epoch(model,device,valid_loader,criterion)

        train_loss = train_loss / len(train_loader)
        train_acc = train_correct / len(train_loader) * 100
        valid_loss = valid_loss / len(valid_loader)
        valid_acc = valid_correct / len(valid_loader) * 100

        print("Epoch:{}/{} AVG Training Loss:{:.3f} AVG Valid Loss:{:.3f} AVG Training Acc {:.2f} % AVG Valid Acc {:.2f} %".format(epoch + 1,
                                                                                                            num_epochs,
                                                                                                            train_loss,
                                                                                                            valid_loss,
                                                                                                            train_acc,
                                                                                                            valid_acc))
        history['train_loss'].append(train_loss)
        history['valid_loss'].append(valid_loss)
        history['train_acc'].append(train_acc)
        history['valid_acc'].append(valid_acc)
from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator
from ignite.metrics import Accuracy, Loss, RunningAverage

    foldperf['fold{}'.format(fold+1)] = history  

torch.save(model,'k_cross_CNN.pt')

"""### **Creating Plots**"""

# Commented out IPython magic to ensure Python compatibility.
# dependencies 
# %matplotlib inline
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error
import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt

"""Network Loss & Accuracy over SGD steps"""

# REMINDER THAT IN THE TRAINING AND TEST FUNCTIONS 
# YOU SHOULD BE COLLECTING THE LOSS AND ACC IN ARR
# ALONG WITH CALCULATING ALL OF THIS 
# model should be a str being either compressed or non-compressed 
# References: https://stackoverflow.com/questions/63106109/how-to-display-graphs-of-loss-and-accuracy-on-pytorch-using-matplotlib

def loss_acc_overSGD(sgdSteps, loss, acc, model):
  plt.plot(sgdSteps, loss)
  plt.title("Loss over SGD " + model)
  plt.xlabel('SGD steps')
  plt.ylabel('loss')
  plt.show()

  plt.plot(sgdSteps, acc)
  plt.title("Acc over SGD " + model)
  plt.xlabel('SGD steps')
  plt.ylabel('Acc')
  plt.show()

"""Confusion Matrix"""

# Reference: https://christianbernecker.medium.com/how-to-create-a-confusion-matrix-in-pytorch-38d06a7f04b7

def confusionMatrix (): # insert parameters here
  y_pred = []
  y_true = []

  for inputs, labels in test_loader:
    output = net(inputs)

    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()
    y_pred.extend(output) # Save Prediction
    
    labels = labels.data.cpu().numpy()
    y_true.extend(labels) # Save Truth

  classes = ('1','2','4','7','8')

  cfm = confusion_matrix(y_true, y_pred)
  df_cm = pd.DataFrame(cfm/np.sum(cfm) * 5, index = [i for i in classes], columns = [i for i in classes])

  mse = mean_squared_error(y_true, y_pred)

  print(mse)

  plt.title("Confusion Matrix")
  plt.figure(figsize = (12,7))
  sn.heatmap = (df_cm, annot=True)
  plt.ylabel('True')
  plt.xlabel('Predicted')
  plt.show()

